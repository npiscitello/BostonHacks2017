{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# BostonHacks\n",
    "\n",
    "<center> <img src=\"https://bostonhacks.io/public/img/terrierLogo.png\" style=\"width: 300px;\" /> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Comparative study in Neural Networks\n",
    "\n",
    "Train a Fully-Connected Neural Network, Simple Convolutional Neural Network and Large Convolutional Neural Network using the MNIST dataset (handwritten digits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loading the MNIST dataset in Keras\n",
    "\n",
    "Plot ad hoc mnist instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAAD8CAYAAADub8g7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f66767280b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "# load (downloaded if needed) the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# plot 4 images as gray scale\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[0], cmap = plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[1], cmap = plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[2], cmap = plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[3], cmap = plt.get_cmap('gray'))\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Baseline Model with Multi-Layer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Complete all imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Fix random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Load data and transform it as the input of a Fully-Connected Neural Network\n",
    "\n",
    "That is, <br />\n",
    "The shape of the input data is currently a 28x28 pixel image but <br />\n",
    "Fully-Connected Neural Network needs one row input per data point <br /><br />\n",
    "So, we transform the 28x28 image to a 784 vector for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# flatten 28*28 images to a 784 vector for each image\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# It is almost always a good idea to perform some scaling\n",
    "#  of input values when using neural network models.\n",
    "# Because the scale is well known and well behaved.\n",
    "# Reduces extreme values (extreme values can lead to mis-classification).\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Define the model for the Fully-Connected Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_fcnn_model():\n",
    "    \n",
    "    # Models in Keras can come in two forms â€“ Sequential and via the Functional API.\n",
    "    \n",
    "    # For most deep learning networks that people build,\n",
    "    #  the Sequential model is likely what isl used.\n",
    "    \n",
    "    # It allows us to easily stack layers (and even recurrent layers)\n",
    "    #  of the network in order from input to output.\n",
    "    \n",
    "    # The functional API allows us to build more complicated architectures,\n",
    "    #  but for this workshop we won't be needing it.\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add a fc layer which takes input of num_pixels (784) defined previously\n",
    "    # This layer also has an output of 'num_pixels' features, defined by 'units'\n",
    "    # Follow this layer's neurons with a ReLU activation\n",
    "    model.add(Dense(units = num_pixels, input_dim = num_pixels, activation = 'relu'))\n",
    "    \n",
    "    # Add a fc layer which takes input from the previous layer\n",
    "    # This layer also has an output of 'num_classes' features, defined by 'units'\n",
    "    # Follow this layer's neurons with a softmax activation\n",
    "    #  to get probability predictions\n",
    "    model.add(Dense(units = num_classes, activation = 'softmax'))\n",
    "    \n",
    "    # Compile model, same as initializing model and allocating space to the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer = 'adam',\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Finally, the train and test the model and print a classification error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.2782 - acc: 0.9212 - val_loss: 0.1408 - val_acc: 0.9590\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.1125 - acc: 0.9670 - val_loss: 0.0911 - val_acc: 0.9718\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.0725 - acc: 0.9794 - val_loss: 0.0800 - val_acc: 0.9765\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.0513 - acc: 0.9849 - val_loss: 0.0739 - val_acc: 0.9784\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.0378 - acc: 0.9893 - val_loss: 0.0655 - val_acc: 0.9802\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.0273 - acc: 0.9927 - val_loss: 0.0640 - val_acc: 0.9804\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.0222 - acc: 0.9940 - val_loss: 0.0625 - val_acc: 0.9810\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.0145 - acc: 0.9967 - val_loss: 0.0585 - val_acc: 0.9812\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.0118 - acc: 0.9973 - val_loss: 0.0557 - val_acc: 0.9818\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.0089 - acc: 0.9981 - val_loss: 0.0568 - val_acc: 0.9821\n",
      "Baseline FCNN Error: 1.79%\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = baseline_fcnn_model()\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data = (X_test, y_test),\n",
    "          epochs = 10, batch_size = 200)\n",
    "\n",
    "# evaluate of the model, return (accuracy, loss)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Baseline FCNN Error: %.2f%%\" % (100 - scores[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Complete all imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Fix random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Load data and transform it as the input of a Convolutional Neural Network \n",
    "\n",
    "That is, <br />\n",
    "The shape of the input data is currently a 28x28 pixel image but <br />\n",
    "Convolutional Neural Network needs to know the number of channels, i.e. RGB has 3 channels, greyscale has 1,<br /><br />\n",
    "So, we transform the 28x28 image to a (height, width, channels) vector for each image. <br /><br />\n",
    "Note: (height, width, channels) dimension ordering applies to tensorflow or tf. <br />\n",
    "Keras also supports Theano where the ordering has to be (channels, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape to be [samples][height][width][channel]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize input as before\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Define the model for the Simple Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_cnn_model():\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add a conv layer which takes input of (28, 28, 1) defined previously\n",
    "    #  does a convolution over a 5x5 kernel window\n",
    "    # This layer also has an output of 32 features, defined by 'filters'\n",
    "    # Follow this layer's neurons with a ReLU activation\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (5, 5),\n",
    "                     input_shape = (28, 28, 1), activation = 'relu'))\n",
    "    \n",
    "    # Add a maxpooling of 2x2, i.e. maxpool width by 2 and height by 2\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    \n",
    "    # Add a dropout, this helps us overcome overfitting\n",
    "    # Dropout randomly changes x% of data input with 0's \n",
    "    # In this case, 20%\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Recall how we talked about the input of a fc layer,\n",
    "    #  it cannot be a matrix and but has to be single row per data point\n",
    "    #  so we flatten the matrix output of a conv layer\n",
    "    #  or maxpooling layer into one row\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Add a fc layer which takes input from the previous layer\n",
    "    # This layer also has an output of 128 features, defined by 'filters'\n",
    "    # Follow this layer's neurons with a ReLU activation\n",
    "    model.add(Dense(units = 128, activation='relu'))\n",
    "    \n",
    "    # Add a fc layer which takes input from the previous layer\n",
    "    # This layer also has an output of 'num_classes' features, defined by 'units'\n",
    "    # Follow this layer's neurons with a softmax activation\n",
    "    #  to get probability predictions\n",
    "    model.add(Dense(units = num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile model, same as initializing model and allocating space to the model\n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = 'adam',\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Finally, the train and test the model and print a classification error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 25s - loss: 0.2235 - acc: 0.9363 - val_loss: 0.0744 - val_acc: 0.9776\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 26s - loss: 0.0713 - acc: 0.9784 - val_loss: 0.0472 - val_acc: 0.9840\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 25s - loss: 0.0509 - acc: 0.9844 - val_loss: 0.0433 - val_acc: 0.9852\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 25s - loss: 0.0404 - acc: 0.9872 - val_loss: 0.0387 - val_acc: 0.9873\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 25s - loss: 0.0323 - acc: 0.9900 - val_loss: 0.0339 - val_acc: 0.9892\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 25s - loss: 0.0263 - acc: 0.9918 - val_loss: 0.0329 - val_acc: 0.9901\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 25s - loss: 0.0221 - acc: 0.9927 - val_loss: 0.0351 - val_acc: 0.9889\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 25s - loss: 0.0191 - acc: 0.9939 - val_loss: 0.0332 - val_acc: 0.9892\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 27s - loss: 0.0162 - acc: 0.9950 - val_loss: 0.0295 - val_acc: 0.9892\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 27s - loss: 0.0132 - acc: 0.9960 - val_loss: 0.0287 - val_acc: 0.9914\n",
      "Baseline CNN Error: 0.86%\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = baseline_cnn_model()\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data = (X_test, y_test),\n",
    "          epochs = 10, batch_size = 200)\n",
    "\n",
    "# evaluate of the model, return (accuracy, loss)\n",
    "scores = model.evaluate(X_test, y_test, verbose = 0)\n",
    "\n",
    "print(\"Baseline CNN Error: %.2f%%\" % (100 - scores[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Large Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Complete all imports \n",
    "#### Fix random seed for reproducibility\n",
    "### Load data and transform it as the input of a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Larger CNN for the MNIST Dataset\n",
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape to be [samples][height][width][channel]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Define the model for the Fully-Connected Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def larger_model():\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add a conv layer which takes input of (28, 28, 1) defined previously\n",
    "    #  does a convolution over a 5x5 kernel window\n",
    "    # This layer also has an output of 32 features, defined by 'filters'\n",
    "    # Follow this layer's neurons with a ReLU activation\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (5, 5), \n",
    "                     input_shape = (28, 28, 1), activation = 'relu'))\n",
    "    \n",
    "    # Add a maxpooling of 2x2, i.e. maxpool width by 2 and height by 2\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    \n",
    "    # Add a conv layer which takes input from the previous layer\n",
    "    #  does a convolution over a 3x3 kernel window\n",
    "    # This layer also has an output of 16 features, defined by 'filters'\n",
    "    # Follow this layer's neurons with a ReLU activation\n",
    "    model.add(Conv2D(filters = 16, kernel_size = (3, 3), activation = 'relu'))\n",
    "    \n",
    "    # Add a maxpooling of 2x2, i.e. maxpool width by 2 and height by 2\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    \n",
    "    # Add a dropout, this helps us overcome overfitting\n",
    "    # Dropout randomly changes x% of data input with 0's \n",
    "    # In this case, 20%\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Add a flatten layer\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Add a fc layer which takes input from the previous layer\n",
    "    # This layer also has an output of 128 features, defined by 'filters'\n",
    "    # Follow this layer's neurons with a ReLU activation\n",
    "    model.add(Dense(units = 128, activation='relu'))\n",
    "    \n",
    "    # Add a fc layer which takes input from the previous layer\n",
    "    # This layer also has an output of 64 features, defined by 'filters'\n",
    "    # Follow this layer's neurons with a ReLU activation\n",
    "    model.add(Dense(units = 64, activation='relu'))\n",
    "    \n",
    "    # Add a fc layer which takes input from the previous layer\n",
    "    # This layer also has an output of 'num_classes' features, defined by 'units'\n",
    "    # Follow this layer's neurons with a softmax activation\n",
    "    #  to get probability predictions\n",
    "    model.add(Dense(units = num_classes, activation = 'softmax'))\n",
    "    \n",
    "    # Compile model, same as initializing model and allocating space to the model\n",
    "    model.compile(loss = 'categorical_crossentropy', \n",
    "                  optimizer = 'adam',\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Finally, the train and test the model and print a classification error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 27s - loss: 0.3869 - acc: 0.8795 - val_loss: 0.0912 - val_acc: 0.9728\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 26s - loss: 0.1051 - acc: 0.9683 - val_loss: 0.0560 - val_acc: 0.9826\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 26s - loss: 0.0747 - acc: 0.9768 - val_loss: 0.0417 - val_acc: 0.9861\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 26s - loss: 0.0596 - acc: 0.9815 - val_loss: 0.0357 - val_acc: 0.9893\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 26s - loss: 0.0496 - acc: 0.9846 - val_loss: 0.0363 - val_acc: 0.9881\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 26s - loss: 0.0446 - acc: 0.9858 - val_loss: 0.0296 - val_acc: 0.9911\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 27s - loss: 0.0382 - acc: 0.9876 - val_loss: 0.0347 - val_acc: 0.9880\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 26s - loss: 0.0346 - acc: 0.9891 - val_loss: 0.0295 - val_acc: 0.9894\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 26s - loss: 0.0313 - acc: 0.9900 - val_loss: 0.0255 - val_acc: 0.9910\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 26s - loss: 0.0279 - acc: 0.9911 - val_loss: 0.0280 - val_acc: 0.9898\n",
      "Large CNN Error: 1.02%\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = larger_model()\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data = (X_test, y_test),\n",
    "          epochs = 10, batch_size = 200)\n",
    "\n",
    "# evaluate of the model, return (accuracy, loss)\n",
    "scores = model.evaluate(X_test, y_test, verbose = 0)\n",
    "\n",
    "print(\"Large CNN Error: %.2f%%\" % (100 - scores[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
